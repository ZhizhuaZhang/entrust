{"tagline":"Trust-Region Optimization in Matlab (allows for bound constraints on design variables)","note":"Don't delete this file! It's used internally to help with page regeneration.","name":"Entrust","google":"","body":"***\r\n<center>ENTRUST</center>\r\n<center>Optimization Using Trust Region Methods</center>\r\n***\r\n\r\n**ENTRUST** is a library of MATLAB routines which implement a variety of algorithms for finding a vector _X_ which minimizes the value of a scalar function _F(X)_.\r\n\r\n**ENTRUST** is a driver for the solution of an unconstrained optimization problem using line search or trust-region globalization strategies and several types of secant update strategies.\r\n\r\n**ENTRUST** also includes the capability to handle least-squares problems, in which a vector _X_ is sought which minimizes the sum of squares of the components of a nonlinear vector function _R(X)_.\r\n\r\nBoth optimization and nonlinear minimization can also be performed with \"box constraints,\" which require the solution to lie within a specified rectangular region. These constraints are incorporated into the trust-region algorithm.\r\n\r\n## The Scalar Optimization Problem\r\n\r\nFor problems in scalar optimization, we're seeking a vector _X_ which minimizes the value of the scalar functional _F(X)_.\r\n\r\nRelated versions of this problem try to maximize _F(X)_, or restrict the range of acceptable _X_ to some rectangular region. **ENTRUST** can handle these related problems if the user requests them.\r\n\r\nTo specify an optimization problem, we assume we have\r\n\r\n>     _X_, an N-vector of independent variables;\r\n>     _F(X)_, a scalar functional;\r\n>     _G(X)_, the gradient N-vector dF/dXi;\r\n>     _H(X)_, the Hessian NxN-matrix d2F/dXidXj.\r\n\r\nand, given a starting estimate _X0_ for the minimizer, we seek a vector _X*_ which minimizes the value _F(X*)_.\r\n\r\n## Linear and Nonlinear Least Squares\r\n\r\nIn least-squares problems, we're seeking a vector _X_ which minimizes the Euclidean norm of a vector of functions _R(X)_. The dimension of _X_ is denoted by N whereas the dimension of _R_ is denoted by M. Often M is larger, even much larger, than N.\r\n\r\nA common source of such problems occurs in data fitting. For example, we might seek the coefficients _A_ and _B_ of a linear model - a line in the plane that comes closest to M pairs of data values. Here N will be 2 (the two unknown coefficients), but M, the number of data pairs, can be much larger. This generalizes to polynomial models; a polynomial of degree N-1 requires N unknown coefficients, which will play the role of the vector _X_. In such cases, the residual vector _R(X)_ is actually linear in the parameters, and we have a linear least squares problem.\r\n\r\nTo specify a problem in least squares, we assume we have:\r\n\r\n>     _X_, an N-vector of independent variables;\r\n>     _R(X)_, an M-vector of nonlinear functions;\r\n>     _J(X)_, the Jacobian MxN-matrix dFi/dXj.\r\n\r\nand, given a starting estimate _X0_ of the minimizer, we seek a vector _X*_ which minimizes the sum of the squares of the entries of _R(X*)_.\r\n\r\n## Optimization versus Least Squares\r\n\r\nThe Optimization and Nonlinear Least Squares problems are related, though not equivalent.\r\n\r\nIf we have a least squares problem (_R_ and _J_), we can create a corresponding optimization problem. We simply define\r\n\r\n    _F(X)_ = sum ( 1 <= i <= M ) _Ri_(_X_)2 \r\n\r\nCertainly, minimizing _F_ will minimize the sum of squares of the entries of _R_.\r\n\r\nIf we have an optimization problem (_F_, _G_ and _H_), we can create a related least squares problem. We simply define\r\n\r\n    _R_(_X_) = _G_(_X_) \r\n\r\nNote that for this least squares problem, we will have M=N. We minimize the norm of _R_(_X_), and if this minimum norm is zero, we have a critical point of our optimization function _F_; however, the critical point is not a guaranteed minimizer of _F_, though for some problems, there is extra information that can tell us we have, indeed, found a minimizer.\r\n\r\n## Solving a Problem using **ENTRUST**\r\n\r\nTo solve a problem, the user must first encode the information that defines the problem.\r\n\r\nFor an optimization problem, write a MATLAB \"FGH\" M-file of the form\r\n\r\n`   [ f, g, H ] = fname ( x, flag )`\r\n      \r\nwhich returns the values of the optimization function, gradient vector, and Hessian matrix evaluated at _x_. (The input value flag should generally be defined to be the empty value, that is, \"flag=[]\".) The value of the Hessian is only needed when a Newton method is being used. Otherwise, the M-file does not need to compute the Hessian, and can simply return an empty value.\r\n\r\nFor a least squares problem, write a MATLAB \"RJ\" M-file of the form\r\n\r\n`   [ r, J ] = fname ( x, flag )`\r\n      \r\nwhich returns the values of the nonlinear functions and the Jacobian evaluated at _x_. (The input value flag should generally be defined to be the empty value, that is, \"flag=[]\".)\r\n\r\nThe user must also set an initial value for the solution estimate _X0_. If no estimate is known, it is acceptable to set _X0_ to zero, but note that changing the initial value can cause the optimization code to converge quickly, very slowly, or to fail to converge. In some cases, changing the initial value can mean that the program converges, but to an entirely different solution with a different optimal value.\r\n\r\nOnce the problem definition has been taken care of, it is necessary to choose a solution algorithm, and, if desired, to change some of the solution parameters from their default value. All information about the solution algorithm is contained in a single MATLAB structure called options, and any choice the user makes is recorded in this single quantity. The first thing the user should do is \"zero out\" this quantity, so that it starts with all default values:\r\n\r\n`    options = [];`\r\n\r\n**ENTRUST** provides four solution algorithms for the user:\r\n\r\n>     'newton', (optimization only), iteratively modify _X_ by solving _H_*_dX_=-_G_;\r\n>     'secant', (optimization only), same as newton, but _J_ or _H_ is approximated by finite differences. (This is the default method.)\r\n>     'steepest_descent', iteratively modify _X_ by solving for the direction of steepest descent, and performing line search to determine a good step size;\r\n>     'gauss_newton', (nonlinear least squares only), given M functions _R_(_X_) in N variables, seek to minimize _F=1/2*R'*R_ by iteratively computing _J'*J*dX=-J'*R_; converges when minimum value of _F_ is zero.\r\n\r\nTo choose a solution algorithm, the user sets the method component of the options structure. A typical command would be:\r\n\r\n`    options.method = 'newton'; `\r\n\r\nIt's probably a good idea to try to run the problem once without specifying any of the other components of options. The results of the first run, if unsatisfactory, can then be used as a guide for determining how to adjust some values of options to get a better result. Typical values that might be changed on a second run would involve requesting that step by step details be printed out:\r\n\r\n`        options.verbose = 1; `\r\n\r\nincreasing the number of times the function routine can be evaluated:\r\n\r\n`        options.max_fevals = 50; `\r\n\r\nlowering the tolerance on the norm of the gradient vector, to ask for a tighter convergence criterion:\r\n\r\n`        options.gradient_tolerance = 0.00000001; `\r\n\r\nAnd of course, there are a number of other parameters that may be adjusted.\r\n\r\n## Example:\r\n\r\nTo seek a minimizer for\r\n\r\n>    f(x) = (x1-2)4+((x1-2)*x2)2+(x2+1)2 \r\n\r\nwe write an FGH routine of the form:\r\n\r\n`      function [ f, g, H ] = test_fgh ( x, flag )\r\n\r\n      f = ( x(1) - 2 )^4...\r\n        + ( ( x(1) - 2 ) * x(2) )^2...\r\n        + ( x(2) + 1 )^2;\r\n\r\n      g(1) = 4 * ( x(1) - 2 )^3 ...\r\n           + 2 * ( x(1) - 2 ) * x(2)^2;\r\n\r\n      g(2) = 2 * ( x(1) - 2 )^2 * x(2)...\r\n           + 2 * ( x(2) + 1 );\r\n\r\n      H = []; `\r\n      \r\n\r\nWe are setting the Hessian matrix _H_ to an empty value. We do not intend to use a Newton method, so the Hessian is not needed.\r\n\r\nAssuming that our starting point is (1,2), we now issue the following commands:\r\n\r\n`        fname = 'test_fgh';\r\n\r\n        x0 = [ 1; 2 ];\r\n\r\n        options = [];\r\n        options.method = 'secant';\r\n\r\n        x = entrust ( fname, x0, options );`\r\n      \r\n\r\nNote that you can also use the \"at sign\" to refer to the function directly:\r\n\r\n`        x = entrust ( @test_fgh, x0, options );`\r\n      \r\n\r\n## The options structure:\r\n\r\nENTRUST has a default solution procedure, but it allows the user to control the computation by overriding the default values of solution parameters. The solution procedure is specified in a MATLAB structure called options, which is the third input argument.\r\n\r\nThe user should always initialize options to its default value, by a command like:\r\n\r\n\r\n`        options = [];`\r\n      \r\n\r\nThen the user can specify particular solution parameters by assigning components of options. For instance, options has a component called options.method that specifies the algorithm to be used. This defaults to 'secant'. Here is how a different algorithm can be chosen:\r\n\r\n\r\n`        options.method = 'newton';`\r\n      \r\n\r\nThe most common components to be changed include the method, gradient_tolerance, max_iterations and max_fevals. The scripts that run the examples include many cases where components are varied.\r\n\r\nGeneral options:\r\n\r\noptions.free_g\r\n    Flag that determines whether or not the gradient G is calculated simultaneously with F;\r\n    default: options.free_g = 1; \r\noptions.goal\r\n    Goal of the problem, supported options include: 'minimization' and 'maximization';\r\n    default: options.goal = 'minimization'; \r\noptions.method\r\n    Type of optimization method, supported options include: 'newton', 'secant', 'steepest_descent', and 'gauss_newton';\r\n    default: options.method = 'secant'; \r\noptions.scale_x\r\n    a vector of length (n_desvar) which contains typical values for the design parameters;\r\n    default: options.scale_x = ones(n_desvar,1); \r\noptions.scale_f\r\n    A typical magnitude of the objective function;\r\n    default: options.scale_f = 1.0; \r\noptions.verbose\r\n    Output flag 0 - no output, 1 - print output;\r\n    default: options.verbose = 1; \r\noptions.x_lower\r\n    a vector of length (n_desvar) which contains lower bounds for the design variables;\r\n    default: options.x_lower =-realmax; \r\noptions.x_upper\r\n    a vector of length (n_desvar) which contains upper bounds for the design variables;\r\n    default: options.x_upper = realmax; \r\n\r\nStopping criteria:\r\n\r\noptions.gradient_tolerance\r\n    Value of the gradient for which convergence is declared;\r\n    default: options.gradient_tolerance = 0.00001; \r\noptions.max_iterations\r\n    Maximum number of main iterations in the optimization;\r\n    default: options.max_iterations = 10; \r\noptions.max_fevals\r\n    Maximum number of calls to 'fname';\r\n    default: options.max_fevals = 30; \r\noptions.step_tolerance\r\n    Value of the change in parameter values for which convergence is declared. May not occur at a local minimum;\r\n    default: options.step_tolerance = 0.00001; \r\n\r\nGlobalization criteria:\r\n\r\noptions.globalization\r\n    Method used to perform globalization, supported options include: 'line_search', 'trust_region' and 'none';\r\n    default: options.globalization = 'none'; \r\noptions.max_step\r\n    Maximum 'trust_region' radius or line search step; \r\noptions.alpha\r\n    (for 'line_search' globalization);\r\n    default: options.alpha = 0.0001, see D&S, p. 126; \r\noptions.tr_radius\r\n    (for 'trust_region' globalization) Initial 'trust_region' radius;\r\n    default: obtained through Cauchy step; \r\n\r\n## Examples:\r\n\r\nThere are a number of example problems available. Most are available in two versions, as an optimization \"FGH\" problem and as a nonlinear least squares \"RJ\" problem:\r\n\r\n    1.) M = N = 2, \"Dennis and Schnabel #1\".\r\n    2.) M = N = 2, \"Dennis and Schnabel #2\".\r\n    3.) M = 3, N = 1, \"Dennis and Schnabel #3\".\r\n    4.) M = 2, N = 2, \"Himmelblau function\".\r\n    5.) M = N = 2, \"the Rosenbrock banana function\"\r\n    6.) M = N = arbitrary multiple of 2, \"the extended Rosenbrock function\".\r\n    7.) M = N = 3, \"the helical valley function\".\r\n    8.) M = N = arbitrary multiple of 4, \"the extended Powell singular function\"\r\n    9.) M = N = arbitrary, \"the trigonometric function\".\r\n    10.) M = 6, N = 4, \"the Wood function\".\r\n    11.) M = N = 3, \"the box-constrained quartic\"\r\n    12.) M = 3, N = 2, \"the Beale function\"\r\n    13.) M = 2, N = 2, \"the localmin function\"\r\n    14.) M = N = 3, \"polynomial\".\r\n    15.) M = N = 2, \"a test case\".\r\n\r\n\r\nAuthor:\r\n\r\nJeff Borggaard,\r\nGene Cliff,\r\nVirginia Tech.\r\nReference:\r\n\r\n    John Dennis, Robert Schnabel,\r\n    Numerical Methods for Unconstrained Optimization and Nonlinear Equations,\r\n    SIAM, 1996,\r\n    ISBN13: 978-0-898713-64-0,\r\n    LC: QA402.5.D44.\r\n    David Himmelblau,\r\n    Applied Nonlinear Programming,\r\n    McGraw Hill, 1972,\r\n    ISBN13: 978-0070289215,\r\n    LC: T57.8.H55.\r\n    Jorge More, Burton Garbow, Kenneth Hillstrom,\r\n    Testing unconstrained optimization software,\r\n    ACM Transactions on Mathematical Software,\r\n    Volume 7, Number 1, March 1981, pages 17-41.\r\n    Jorge More, Burton Garbow, Kenneth Hillstrom,\r\n    Algorithm 566: FORTRAN Subroutines for Testing unconstrained optimization software,\r\n    ACM Transactions on Mathematical Software,\r\n    Volume 7, Number 1, March 1981, pages 136-140.\r\n\r\nSource Code:\r\n\r\n    entrust.m the optimization code.\r\n\r\nExamples and Tests:\r\n\r\nOPT01 is Dennis and Schnabel example 1, with M=N=2.\r\n\r\n    opt01_fgh.m evaluates F, G and H for optimization.\r\n    opt01_rj.m evaluates R and J for least squares.\r\n    opt01_run.m calls ENTRUST to solve the optimization and nonlinear least squares problems.\r\n    opt01_output.txt the output file.\r\n\r\nOPT02 is Dennis and Schnabel example 2, with M=N=2.\r\n\r\n    opt02_fgh.m evaluates F, G and H for optimization.\r\n    opt02_rj.m evaluates R and J for least squares.\r\n    opt02_run.m calls ENTRUST to solve the optimization and nonlinear least squares problems.\r\n    opt02_output.txt the output file.\r\n\r\nOPT03 is Dennis and Schnabel example 3 with M=3, N=1. It includes a parameter which allows the problem to be changed so that the optimum value of F is zero or nonzero. This demonstrates how the Gauss-Newton procedure can be made to fail.\r\n\r\n    opt03_fgh.m evaluates F, G and H for optimization.\r\n    opt03_rj.m evaluates R and J for least squares.\r\n    opt03_run.m calls ENTRUST to solve the optimization and nonlinear least squares problems.\r\n    opt03_output.txt the output file.\r\n\r\nOPT04 is the Himmelblau function, with M=2, N=2. The function has four global minima, where F=0. In the demonstration, the Newton method goes to one minimum, the secant method fails, and the Gauss-Newton method reaches a different minimum from the same initial point.\r\n\r\n    opt04_fgh.m evaluates F, G and H for optimization.\r\n    opt04_rj.m evaluates R and J for least squares.\r\n    opt04_run.m calls ENTRUST to solve the optimization and nonlinear least squares problems.\r\n    opt04_output.txt the output file.\r\n\r\nOPT05 is the Rosenbrock \"banana\" function, with M = N = 2.\r\n\r\n    opt05_fgh.m evaluates F, G and H for optimization.\r\n    opt05_rj.m evaluates R and J for least squares.\r\n    opt05_run.m calls ENTRUST to solve the optimization and nonlinear least squares problems.\r\n    opt05_output.txt the output file.\r\n\r\nOPT06 is the extended Rosenbrock \"banana\" function, with M=N=arbitary multiple of 2.\r\n\r\n    opt06_fgh.m evaluates F, G and H for optimization.\r\n    opt06_rj.m evaluates R and J for least squares.\r\n    opt06_run.m calls ENTRUST to solve the optimization and nonlinear least squares problems.\r\n    opt06_output.txt the output file.\r\n\r\nOPT07 is the helical valley function, with M=N=3.\r\n\r\n    opt07_fgh.m evaluates F, G and H for optimization.\r\n    opt07_rj.m evaluates R and J for least squares.\r\n    opt07_run.m calls ENTRUST to solve the optimization and nonlinear least squares problems.\r\n    opt07_output.txt the output file.\r\n\r\nOPT08 is the extended Powell singular function, with M=N=an arbitrary multiple of 4.\r\n\r\n    opt08_fgh.m evaluates F, G and H for optimization.\r\n    opt08_rj.m evaluates R and J for least squares.\r\n    opt08_run.m calls ENTRUST to solve the optimization and nonlinear least squares problems.\r\n    opt08_output.txt the output file.\r\n\r\nOPT09 is the trigonometric function, with M=N=arbitrary.\r\n\r\n    opt09_fgh.m evaluates F, G and H for optimization.\r\n    opt09_rj.m evaluates R and J for least squares.\r\n    opt09_run.m calls ENTRUST to solve the optimization and nonlinear least squares problems.\r\n    opt09_output.txt the output file.\r\n\r\nOPT10 is the Wood function, with M=6 and N=4.\r\n\r\n    opt10_fgh.m evaluates F, G and H for optimization.\r\n    opt10_rj.m evaluates R and J for least squares.\r\n    opt10_run.m calls ENTRUST to solve the optimization and nonlinear least squares problems.\r\n    opt10_output.txt the output file.\r\n\r\nOPT11 is the box-constrained quartic, with M=N=3.\r\n\r\n    opt11_fgh.m evaluates F, G and H for optimization.\r\n    opt11_rj.m evaluates R and J for least squares.\r\n    opt11_run.m calls ENTRUST to solve the optimization and nonlinear least squares problems.\r\n    opt11_output.txt the output file.\r\n\r\nOPT12 is the Beale function, with M=3 and N=2. Convergence is relatively easy with the starting point [1,1], but hard to achieve with [1,4].\r\n\r\n    opt12_fgh.m evaluates F, G and H for optimization.\r\n    opt12_rj.m evaluates R and J for least squares.\r\n    opt12_run.m calls ENTRUST to solve the optimization and nonlinear least squares problems.\r\n    opt12_output.txt the output file.\r\n\r\nOPT13 is a polynomial example, with M=N=2. It has a local minimum for which F is about 5, and a global minimum for which F is 0.\r\n\r\n    opt13_fgh.m evaluates F, G and H for optimization.\r\n    opt13_rj.m evaluates R and J for least squares.\r\n    opt13_run.m calls ENTRUST to solve the optimization and nonlinear least squares problems.\r\n    opt13_output.txt the output file.\r\n\r\nOPT14 is a polynomial example, with M=N=3.\r\n\r\n    opt14_fgh.m evaluates F, G and H for optimization.\r\n    opt14_rj.m evaluates R and J for least squares.\r\n    opt14_run.m calls ENTRUST to solve the optimization and nonlinear least squares problems.\r\n    opt14_output.txt the output file.\r\n\r\nOPT15 is a test case, with M=N=2.\r\n\r\n    opt15_fgh.m evaluates F, G and H for optimization.\r\n    opt15_run.m calls ENTRUST to solve the optimization problem.\r\n    opt15_output.txt the output file.\r\n\r\n## Related Data and Programs:\r\n\r\nASA047 is a FORTRAN77 library which minimizes a scalar function of several variables using the Nelder-Mead algorithm.\r\n\r\nDQED a FORTRAN90 library which solves constrained least squares problems.\r\n\r\nCOORDINATE_SEARCH is a MATLAB program which minimizes a scalar function of several variables using the coordinate search algorithm.\r\n\r\nfminsearch is a MATLAB built in command which minimizes a scalar function of several variables using the Nelder-Mead algorithm.\r\n\r\nMINPACK a FORTRAN90 library which solves systems of nonlinear equations, or the least squares minimization of the residual of a set of linear or nonlinear equations.\r\n\r\nNELDER_MEAD a MATLAB program which minimizes a scalar function of multiple variables using the Nelder-Mead algorithm.\r\n\r\nNL2SOL a FORTRAN90 library which implements an adaptive nonlinear least-squares algorithm.\r\n\r\nPRAXIS a FORTRAN90 library which minimizes a scalar function of several variables, without derivative information.\r\n\r\nTEST_NLS a FORTRAN90 library which defines test problems requiring the least squares minimization of a set of nonlinear functions.\r\n\r\nTEST_OPT a FORTRAN90 library which defines test problems requiring the minimization of a scalar function of several variables.\r\n\r\nTOMS178 a MATLAB library which optimizes a scalar functional of multiple variables using the Hooke-Jeeves method.\r\n\r\nTOMS611 a FORTRAN90 library which optimizes a scalar functional of multiple variables.\r\n\r\nUNCMIN is a FORTRAN77 library which can be used to seek the minimizer of a scalar functional of multiple variables."}